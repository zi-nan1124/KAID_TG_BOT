from SOLONA.LIB.common import *
from SOLONA.LIB.HeliusLogDecoder import HeliusLogDecoder
from SOLONA.LIB.PumpFunTradeDecoder import PumpFunTradeDecoder
from SOLONA.LIB.MySolanaClient import MySolanaClient
from multiprocessing import Pool, cpu_count
import asyncio


def decode_single(tx_json):

    decoder = PumpFunTradeDecoder()
    try:
        tx_data = tx_json.get("result")
        if not tx_data:
            Logger().error("tx_json ä¸­ç¼ºå°‘ 'result' å­—æ®µ")
            return None

        df = decoder.decode(tx_data)
        if df is not None and not df.empty:
            return df
    except Exception as e:
        Logger().error(f"è§£æäº¤æ˜“å¤±è´¥: {e}")
    return None


class TransactionListDecoder:
    def __init__(self):
        self.helius = HeliusLogDecoder()
        self.pump_decoder = PumpFunTradeDecoder()

        # æ„å»ºåŠ æƒ RPC URL æ± ï¼ˆä»˜è´¹èŠ‚ç‚¹æƒé‡ä¸º4ï¼‰
        self.rpc_pool = [config.CONFIG["rpc_url"]] * 4 + [
            config.CONFIG["rpc_url1"],
            config.CONFIG["rpc_url2"],
            config.CONFIG["rpc_url3"]
        ]
        self.client = Client(config.CONFIG["rpc_url"])

    def parse_signatures(self, sig_df: pd.DataFrame):
        logger.info("å¼€å§‹è§£æç­¾å...")
        signatures = sig_df["signature"].tolist()
        transactions = self.helius.batch_parse_transactions(signatures)

        result_all = [self.helius.summarize(tx) for tx in transactions if tx]
        df_all = pd.DataFrame(result_all)

        df_pump = df_all[df_all["protocol"].isin(["PUMP_FUN", "PUMP_AMM"])].copy()
        df_others = df_all[~df_all["protocol"].isin(["PUMP_FUN", "PUMP_AMM"])].copy()

        logger.info(f"å…±è§£æäº¤æ˜“ {len(df_all)} æ¡ï¼Œå…¶ä¸­ PUMP ç±»å‹ {len(df_pump)} æ¡ï¼Œå…¶å®ƒç±»å‹ {len(df_others)} æ¡")
        return df_others, df_pump

    def _get_weighted_rpc_url(self):
        return random.choice(self.rpc_pool)

    async def fetch_pump_tx_json(self, pump_df: pd.DataFrame) -> list:
        logger.info("å¼€å§‹å¼‚æ­¥è·å– PUMP äº¤æ˜“ JSON æ•°æ®ï¼ˆæŒ‰æƒé‡åˆ†é… RPCï¼‰...")

        signatures = pump_df["signature"].tolist()
        if not signatures:
            return []

        weighted_rpc_urls = {
            config.CONFIG["rpc_url"]: 4,
            config.CONFIG["rpc_url1"]: 1,
            config.CONFIG["rpc_url2"]: 1,
            config.CONFIG["rpc_url3"]: 1
        }

        total_weight = sum(weighted_rpc_urls.values())
        rpc_sig_map = {}
        remaining = signatures.copy()

        for url, weight in weighted_rpc_urls.items():
            count = int(len(signatures) * weight / total_weight)
            rpc_sig_map[url] = remaining[:count]
            remaining = remaining[count:]

        first_url = next(iter(weighted_rpc_urls))
        rpc_sig_map[first_url].extend(remaining)

        # å¼‚æ­¥è°ƒåº¦
        tasks = []
        for url, sigs in rpc_sig_map.items():
            client = MySolanaClient(url)
            tasks.append(client.get_transaction_by_signature(sigs))

        all_results = await asyncio.gather(*tasks, return_exceptions=True)

        results = []
        for i, result in enumerate(all_results):
            if isinstance(result, Exception):
                logger.error(f"ç¬¬ {i} ä¸ª RPC è·å–å¤±è´¥: {result}")
                continue
            valid = [tx for tx in result if "result" in tx]
            logger.info(f"ç¬¬ {i} ä¸ª RPC æˆåŠŸè·å– {len(valid)} æ¡äº¤æ˜“")
            results.extend(valid)

        return results

    def decode_pump_multithreaded(self, tx_json_list: list) -> pd.DataFrame:
        logger.info("å¼€å§‹å¤šè¿›ç¨‹è§£æ PUMP äº¤æ˜“...")
        with Pool(processes=min(cpu_count(), 8)) as pool:
            results = pool.map(decode_single, tx_json_list)

        results = [df for df in results if df is not None]
        if results:
            final_df = pd.concat(results, ignore_index=True)
            logger.info(f"æˆåŠŸè§£æ {len(final_df)} æ¡ PUMP äº¤æ˜“")
            return final_df
        else:
            logger.warn("æœªè§£æå‡ºä»»ä½• PUMP äº¤æ˜“")
            return pd.DataFrame()

    def merge_results(self, df_others: pd.DataFrame, df_pump: pd.DataFrame) -> pd.DataFrame:
        logger.info("å¼€å§‹åˆå¹¶ç»“æœ...")
        df_all = pd.concat([df_others, df_pump], ignore_index=True)
        if "timestamp" in df_all.columns:
            df_all = df_all.sort_values(by="timestamp", ascending=True)
        else:
            logger.warn("ç»“æœä¸­ç¼ºå°‘ timestamp å­—æ®µï¼Œæ— æ³•æ’åº")
        return df_all.reset_index(drop=True)

    async def decode(self, sig_df: pd.DataFrame) -> pd.DataFrame:
        logger.info("å¼€å§‹å…¨æµç¨‹è§£æï¼ˆå¼‚æ­¥ï¼‰...")
        df_others, df_pump_init = self.parse_signatures(sig_df)
        tx_jsons = await self.fetch_pump_tx_json(df_pump_init)
        df_pump_decoded = self.decode_pump_multithreaded(tx_jsons)  # ä¿ç•™ CPU å¤šè¿›ç¨‹è§£æ
        df_all = self.merge_results(df_others, df_pump_decoded)
        return df_all



async def run_test_from_csv(csv_path="test.csv", output_path="parsed_transactions.csv"):
    # è¯»å–ç­¾å CSV æ–‡ä»¶
    df_signatures = pd.read_csv(csv_path)
    if "signature" not in df_signatures.columns:
        print("[âŒ] CSV æ–‡ä»¶ç¼ºå°‘ signature åˆ—")
        return

    pd.set_option("display.max_columns", None)
    pd.set_option("display.width", 200)
    pd.set_option("display.float_format", '{:.9f}'.format)

    num_sigs = len(df_signatures)
    print(f"[â±ï¸] å¼€å§‹è§£æ {num_sigs} æ¡ç­¾åäº¤æ˜“...")
    start_time = time.time()

    decoder = TransactionListDecoder()
    final_result = await decoder.decode(df_signatures)

    elapsed = time.time() - start_time
    print(f"[âœ…] è§£æå®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f} ç§’")

    # è¾“å‡ºç®€è¦ä¿¡æ¯
    print(f"[ğŸ“Š] æ€»è®°å½•æ•°: {len(final_result)}")
    if not final_result.empty and "protocol" in final_result.columns:
        print(f"[ğŸ“¦] æ¶‰åŠåè®®: {final_result['protocol'].unique().tolist()}")

    final_result.to_csv(output_path, index=False)
    print(f"[ğŸ’¾] å·²ä¿å­˜è‡³: {output_path}")

    print("\n--- éƒ¨åˆ†è®°å½•é¢„è§ˆ ---")
    print(final_result.head(5))
    return final_result


if __name__ == "__main__":
    asyncio.run(run_test_from_csv())



